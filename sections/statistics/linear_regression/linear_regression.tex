\documentclass[12pt]{article}

% loads config.sty
\makeatletter
\def\input@path{{../../../}}
\makeatother
\usepackage{config}
\usepackage{macros}

\title{Linear Regression}
\date{\today}

\begin{document}

\maketitle

\section{Generalized Least Squares (GLS)}

Largely adapted from the \href{https://en.wikipedia.org/wiki/Generalized_least_squares}{respective Wikipedia article}.

\subsection{Model definition}

In Generalized Least Squares (GLS), we define an outcome's mean to be a linear function of a set of predictors:
$$ y = X \beta + \epsilon $$

Where:

\begin{itemize}
    \item $y$: outcome vector. $N \times 1$
    \begin{itemize}
        \item $N$: number of samples
    \end{itemize}
    \item $X$: design matrix. $N \times K$ matrix
    \begin{itemize}
        \item $K$: number of predictors
    \end{itemize}
    \item $\beta$ : coefficients vector. $K \times 1$ vector
    \item $\epsilon$ : error term. $N \times 1$ vector
\end{itemize}

Because of our conditional mean definition earlier, the error term, $\epsilon$, has a mean of zero for a given set of predictor values ($X$):
$$ E [\epsilon | X ] = 0 $$

We also assume that the variance of the error term given $X$ is described by an invertible $N \times N$ covariance matrix, $\Omega$:
$$ Cov[ \epsilon | X ] = \Omega $$

We further assume that the error term follows a multivariate normal distribution with mean 0 and covariance $\Omega$:
$$ \epsilon \sim \mathcal{N} (0, \Omega) $$

And so it follows that:
$$ y \sim \mathcal{N} (X \beta, \Omega) $$





\subsection[Least squares estimate of beta]{Least squares estimate of $\beta$}

We denote a candidate estimate for the $\beta$ vector as $b$ and define its residual vector as $ y - X b $. The goal of GLS is to find the estimate of $\beta$ that maximizes the likelihood of the data given the above model, which we can calculate using the \href{https://en.wikipedia.org/wiki/Multivariate_normal_distribution}{probability density function of a multivariate normal distribution}:

$$
\frac{1}{\sqrt{(2 \pi)^K |\Sigma| }}
\exp{(-\frac{1}{2}(x - \mu)^{\T} \Sigma^{-1} (x - \mu))}
$$

We can plug in $\Sigma = \Omega$, $x = y$, and $\mu = X b$. The $b$ that maximizes this likelihood function will be the same that minimizes the inside of the exponent (without the negative), which is also the squared Mahalanobis length of the residual vector:

$$ \hat{\beta} = \argmin_b ~ (y - Xb)^{\T} \Omega^{-1} (y - Xb) $$

Through matrix algebra, this is equivalent to:

\begin{align*}
    \hat{\beta} &= \argmin_b ~ (y - Xb)^{\T} \Omega^{-1} (y - Xb) \\
    &= \argmin_b ~ y^{\T} \Omega^{-1} y - y^{\T} \Omega^{-1} Xb - (Xb)^{\T} \Omega^{-1} y + (Xb)^{\T} \Omega^{-1} Xb \\
    &= \argmin_b ~ y^{\T} \Omega^{-1} y + (Xb)^{\T} \Omega^{-1} (Xb) - 2(Xb)^{\T} \Omega^{-1} y
\end{align*}

We can use calculus to solve for the $b$ that minimizes this expression by taking the partial derivative with respect to $b$ and solving for 0:

\begin{align*}
    0 &= \frac{\partial}{\partial b} [ y^{\T} \Omega^{-1} y + (Xb)^{\T} \Omega^{-1} (Xb) - 2(Xb)^{\T} \Omega^{-1} y ]\\
    &=  2X^{\T} \Omega^{-1} X \hat{\beta} - 2X^{\T} \Omega^{-1} y
\end{align*}

Which yields:
$$ \hat{\beta} = (X^{\T} \Omega^{-1} X)^{-1} X^{\T} \Omega^{-1} y $$

\subsubsection{Specific case of Ordinary Least Squares}

Note that in Ordinary Least Squares (OLS), the covariance matrix is an identity matrix, $\Omega = I$. That is, the residuals are uncorrelated with each other. This simplifies the above equation to:

\begin{align*}
    \hat{\beta} &= (X^{\T} \Omega^{-1} X)^{-1} X^{\T} \Omega^{-1} y \\
    &= (X^{\T} I X)^{-1} X^{\T} I^{-1} y \\
    &= (X^{\T} X)^{-1} X^{\T} y
\end{align*}



\subsection[Variance of beta-hat]{Variance of $\hat{\beta}$}

To get the variance of the $\hat{\beta}$ estimate, we only need to focus on the variance of $y$, as all other terms are not random variables. $Var[y] = \Omega$ since $\epsilon$ is independent of $X \beta$, as shown below:

\begin{align*}
    Var[y] &= Var[X \beta + \epsilon] \\
    &= Var[X \beta] + Var[\epsilon] \\
    &= 0 + E[ Var[\epsilon | X] ] + Var[ E[\epsilon | X]] \\
    &= E[ \Omega ] + Var[ 0 ] \\ 
    &= \Omega
\end{align*}

Let's define the scalar $A = (X^{\T} \Omega^{-1} X)^{-1} X^{\T} \Omega^{-1}$, such that $\hat{\beta} = A y$. Furthermore, note that since $\Omega$ is a covariance matrix, it (and its inverse) is symmetric: $\Omega = \Omega^{\T}$. The same symmetry property applies to $X^{\T} \Omega^{-1} X$. We can then solve:

\begin{align*}
    Var[ \hat{\beta} ] &= Var[(X^{\T} \Omega^{-1} X)^{-1} X^{\T} \Omega^{-1} y] \\
    &= Var[A y] \\
    &= A Var[y] A^{\T} \\
    &= A \Omega A^{\T} \\
    &= (X^{\T} \Omega^{-1} X)^{-1} X^{\T} \Omega^{-1} \Omega ((X^{\T} \Omega^{-1} X)^{-1} X^{\T} \Omega^{-1})^{\T} \\
    &= (X^{\T} \Omega^{-1} X)^{-1} X^{\T} \Omega^{-1} \Omega \Omega^{-1} X (X^{\T} \Omega^{-1} X)^{-1} \\
    &= (X^{\T} \Omega^{-1} X)^{-1} (X^{\T} \Omega^{-1} X) (X^{\T} \Omega^{-1} X)^{-1}
\end{align*}

We can define $B = X^{\T} \Omega^{-1} X$, allowing us to simplify the above equation, $B^{-1} B B^{-1} = B^{-1}$, yielding:

$$ Var[ \hat{\beta} ] = (X^{\T} \Omega^{-1} X)^{-1} $$








\end{document}