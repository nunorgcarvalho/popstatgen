\documentclass[12pt]{article}

% loads config.sty
\makeatletter
\def\input@path{{../../../}}
\makeatother
\usepackage{config}
\usepackage{macros}
\usepackage{bookmark}

\title{Maximum Likelihood}
\date{\today}

\begin{document}

\maketitle

A lot of this text is adapted from the textbook [Lynch \& Walsh, 1998], particularly Appendix 4, as well as some back-and-forth with ChatGPT to help consolidate ideas.
The point of this is to re-explain (restricted) maximum likelihood in a way that makes more sense to me, and hopefully to you. Here, I focus more on the theory and abstraction of ML/REML, and in other entries, I plan to apply it to a particular class of problem (e.g. variance component estimation).

\section{Maximum Likelihood}

\subsection{Likelihood}

When we construct a model of some real world phenomenon, we define some process that describes how data, $\by$, is generated according to some set of parameters, $\bTheta$.
Typically, our model is not deterministic since we do not understand every single cause of the real world phenomenon with perfect accuracy.
Instead, our data is generated probabilistically, which means given some set of parameter values, different possible data values can be generated with differing probabilities.
We can also think of the reverse:
Given our observed data, there are many different combinations of parameter values that could have generated that data, each with differing likelihoods.

We can define a \textbf{likelihood function} that tells us the probability density of some data $\by$ being generated by some set of parameter values $\bTheta$:
$$ \ell(\bTheta | \by) $$
Note that because we actually do know what the data is (it is observed), we are asking the question: "How well does an input set of parameter values, $\bTheta$, explain the observed data, $\by$.
This does not necessarily tell us the \textbf{probability} of the parameter values given our data, since we are not computing the likelihood across all possible parameters (weighted by some prior), as we would in Bayesian statistics.

Because our data is (usually) composed of multiple data points, and our model often describes the probability of a single data point being generated, our likelihood for the entire data will often be the product of multiple individual likelihoods:
$$ \ell(\bTheta | \by) = \prod_i^{N} \ell(\bTheta | y_i) $$
Note that this makes the crucial assumption that each data point is independent of each other, such that the likelihood of one data point is not affected by the values of the other datapoints.
Because sums are typically easier to work with than products, it's useful to instead compute the natural logarithm of the likelihood, which per the properties of logarithms, turns those products into sums.
This defines the \textbf{log-likelihood}:
$$ \mathrm{L}(\bTheta | \by) = \sum_i^{N} \ln( \ell(\bTheta | y_i) ) $$

Because logarithms are a monotonic function, it means that whatever parameter values maximize the likelihood also maximize the log-likelihood, and same for the minimum.

\subsubsection{Example of the likelihood of a function}
(Adapted from Example 1 of Appendix 4 of [Lynch \& Walsh, 1998]).
Let's say we have $N$ data points: $\by = [y_1,y_2, ..., y_N]$, which we believe were generated from some normal distribution with a variance of $\sigma^2 = 1$ but an unknown mean.
We know the probability density function (PDF) for a normal distribution, which is a function of the mean $\mu$ and variance $\sigma$.
$$
\Pr(y_i, \mu, \sigma) = 
\frac{1}{\sigma \sqrt{2 \pi}}
\exp[- \frac{1}{2} (
    \frac{y_i - \mu}{\sigma}
    )^2]
$$
In our case, we know $\sigma = 1$ and we also observe the data $\by$.
What we don't know is the mean, $\mu$, which is the only non-fixed parameter.
We can plug in our known parameter $\sigma$, compute the product of the PDF for all our data points, and take the logarithm.
This results in a log-likelihood function that describes how well an inputted $\mu$ value explains our observed data:
$$
\LL(\mu | \by) = - \frac{n}{2} \ln(2 \pi) -
\frac{1}{2} \sum_i^{N} (y_i - \mu)^2
$$

\subsection{Maximum Likelihood Estimation}
Once we have described the likelihood of some set of parameter values given our observed data, we can then ask the question:
What set of parameter values maximizes the likelihood of our data?
The rationale is that if we want to estimate these unknown parameters, our best bet for a single point estimate is the combination of parameter values that are most likely to produce the observed data.
The set of parameter values $\hat{\bTheta}$ that maximizes the likelihood of our observed data is called the \textbf{maximum likelihood estimate (MLE)}.

From calculus, we know that the maximum of a multivariable function can be identified by taking the partial derivatives of the function with respect to each parameter and then solving for the parameter values that make them equal to zero. We define this first partial derivative of a log-likelihood function to be the \textbf{Score} ($S$) of the likelihood function:
$$
S(\bTheta) =
\frac{ \partial \LL (\bTheta)
}{ \partial \bTheta }
$$
Note that if $\bTheta$ is a vector of $M$ parameters, then $S(\bTheta)$ is also a vector of length $M$.
The MLE, $\hat{\bTheta}$, is therefore the solution to the following:
$$ S(\hat{\bTheta}) = 0 $$

\subsubsection{Example of the MLE of a function}
Continuing the previous example, we can compute the score of the likelihood function, which in this case is just a single derivative:
\begin{align*}
S(\mu) &=
\frac{ \partial \LL (\mu)
}{ \partial \mu } \\
&= \frac{ \partial }{ \partial \mu } [
- \frac{n}{2} \ln(2 \pi) -
\frac{1}{2} \sum_i^{N} (y_i - \mu)^2 ] \\
&= \frac{ \partial }{ \partial \mu } [
- \frac{n}{2} \ln(2 \pi) -
\frac{1}{2} \sum_i^{N} (y_i^2 - 2 y_i \mu + \mu^2) ] \\
&= - \frac{1}{2} \sum_i^{N} (- 2 y_i + 2 \mu) \\
&= N ( \bar{y} - \mu)
\end{align*}
where $\bar{y} = \frac{1}{N} \sum_i^N (y_i) $ (i.e. the sample mean). We now set $S(\hat{\mu}) = 0$ and solve:
\begin{align*}
0 &= S(\hat{\mu}) \\
&= N ( \bar{y} - \hat{\mu})
\end{align*}
The solution is: $\hat{\mu} = \bar{y} = \frac{1}{N} \sum_i^N (y_i)$.
Matching our intuitions, the mean parameter that maximizes the likelihood of our data is the arithmetic mean of our data sample.

\end{document}